import requests 
import bs4 as bs 
from time import sleep
from time import time
from random import randint
import pandas as pd 

#10 variables:text content
titles = []
#first paragragh text
first_ps = []
authors = []
#last reply person
reply_lps = []
#reply number
reply_qs = []
#floor in website
data_floors =[]
#creat time
found_times = []
#last reply time
ltimes = []
#fields for subwebsite
fields = []
#total 4 numbers in 10

#loop
intervals = [str(i*50) for i in range(0,75)] 

for interval in intervals:
   
    source = requests.get('https://tieba.baidu.com/f?kw=%E5%AE%B6%E6%9A%B4&ie=utf-8&pn=' + interval)

    sleep(randint(1,74))
    
    page_html = bs.BeautifulSoup(source.content, 'html.parser')
    
    lis_containers = page_html.find_all('li', class_ = 'j_thread_list clearfix') 
    
    for container in lis_containers:
         
        try:
            for title in container.find_all('a', class_ = 'j_th_tit'): 
                titles.append(title.text.strip()) 
        except:
            title = "no_title"
            titles.append(title)
            
        try:
            for first_p in container.find_all('div', class_ = 'threadlist_abs threadlist_abs_onlyline'): 
                first_ps.append(first_p.text.strip()) 
        except:
            first_p = "no_first_p"
            first_ps.append(first_p)
            
        try:
            for author in container.find_all('span', class_ = 'frs-author-name-wrap'): 
                authors.append(author.text.strip()) 
        except:
            author = "no_author"
            authors.append(author)
            
        try:
            for reply_lp in container.find_all('span', class_ = 'tb_icon_author_rely j_replyer'): 
                reply_lps.append(reply_lp.text.strip()) 
        except:
            reply_lp = "no_reply_lp"
            reply_lps.append(reply_lp) 
            
        try: 
            for reply_q in container.find_all('span', class_ = 'threadlist_rep_num center_text'): 
                reply_qs.append(reply_q.text.strip())
        except: 
            reply_q = "no_reply_q"
            reply_qs.append(reply_q)
        
        try:
            for data_floor in container:
                data_floors.append(data_floorget('data-floor')) 
        except:
            data_floor = "no_data_floor"
            data_floors.append(data_floor)
            
        try:
            for found_time in container.find_all('span', class_ = 'pull-right is_show_create_time'): 
                found_times.append(found_time.text.strip()) 
        except:
            found_time = "no_found_time"
            found_times.append(found_time)
            
        try:
            for ltime in container.find_all('span', class_ = 'threadlist_reply_date pull_right j_reply_data'): 
                ltimes.append(ltime.text.strip()) 
        except:
            ltime = "no_ltime"
            ltimes.append(ltime)
            
        try:
            for field in container.find_all('a', class_ = 'j_th_tit'): 
                fields.append(field.get('href').replace('/p/', '')) 
        except:
            field = "no_feild"
            fields.append(feild)


print(len(titles))
print(len(first_ps))

print(len(authors))
print(len(reply_lps))

print(len(reply_qs))
print(len(data_floors))
print(len(found_times))
print(len(ltimes))
print(len(fields))

reply_lps.append('none')
ltimes.append('none')
data_10y = {
    'titles': titles,
    'first_ps': first_ps,
    'authors': authors,
    'reply_lps': reply_lps,
    'reply_qs': reply_qs,
    'data_floors': data_floors,
    'found_times': found_times,
    'ltimes': ltimes,
    'fields': fields}

#Export
pd_CityUnews_10y = pd.DataFrame.from_dict(data_10y)

print(pd_CityUnews_10y.info())
pd_CityUnews_10y.to_csv('Data_Tieba1.csv')

#Visualization
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
df = pd.read_csv("table2.csv")

df[['year',
           'number']].plot(kind='line',
                                  x='year',
                                  y='number',
                                  title='Reply number in recent 7 years',
                                  legend=False)
plt.show()
plt.savefig("LineChart1.jpg")

df[['year',
           'mean']].plot(kind='line',
                                  x='year',
                                  y='mean',
                                  title="Mean of each post's reply number in recent 7 years",
                                  legend=False)
plt.show()
plt.savefig("LineChart2.jpg")

#Word Cloud
import jieba.analyse
import imageio
import jieba.posseg as pseg
def jieba_cut():
    #停用词
    fr = open('/Users/liuziwei/Downloads/语料（附停用词）/stopword.txt', 'r')
    stop_word_list = fr.readlines()
    new_stop_word_list = []
    for stop_word in stop_word_list:
        stop_word = stop_word.replace('\ufeef', '').strip()
        new_stop_word_list.append(stop_word)
    print(stop_word_list)  #输出停用词
    #输出文书网站 词语出现的次数
    fr_xyj=open('/Users/liuziwei/Desktop/文书网.txt','r',encoding='utf-8')
    s=fr_xyj.read()
    words=jieba.cut(s,cut_all=False)
    word_dict={}
    word_list=''
    for word in words:
        if (len(word) > 1 and not word in new_stop_word_list):
            word_list = word_list + ' ' + word
            if (word_dict.get(word)):
                word_dict[word] = word_dict[word] + 1
            else:
                word_dict[word] = 1
    fr.close()
    ##print(word_list)
    #print(word_dict) #输出文书网 词语出现的次数

    #按次数进行排序
    sort_words=sorted(word_dict.items(),key=lambda x:x[1],reverse=True)
    print(sort_words[2:61])#输出前3-50的词

    from wordcloud import WordCloud
    color_mask =imageio.imread("/Users/liuziwei/Downloads/21.png")
    wc = WordCloud(
            background_color="white",  # 背景颜色
            max_words=100,  # 显示最大词数
            font_path="/Users/liuziwei/Library/Group Containers/UBF8T346G9.Office/FontCache/4/CloudFonts/STFangsong/32515377567.ttf",  # 使用字体
            min_font_size=15,
            max_font_size=50,
            width=400,
            height=860,
            mask=color_mask) # 图幅宽度
    i=str('why')
    wc.generate(word_list)
    wc.to_file(str(i)+".png")
jieba_cut()

